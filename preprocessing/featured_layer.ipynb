{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import collections\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurando nível de logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lendo arquivos de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frzd_olist_order_reviews = pd.read_parquet('../dataset/delivery/dlzd_olist_order_reviews.parquet.snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_comment_title_and_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>review_answer_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>[parabens, lojas, lannister, adorei, comprar, ...</td>\n",
       "      <td>[parabens, lojas, lannister, adorei, comprar, ...</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018-03-02 10:26:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8670d52e15e00043ae7de4c01cc2fe06</td>\n",
       "      <td>b9bf720beb4ab3728760088589c62129</td>\n",
       "      <td>4</td>\n",
       "      <td>[recomendo]</td>\n",
       "      <td>[aparelho, eficiente, no, site, marca, do, apa...</td>\n",
       "      <td>[recomendo, aparelho, eficiente, no, site, mar...</td>\n",
       "      <td>2018-05-22</td>\n",
       "      <td>2018-05-23 16:45:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4b49719c8a200003f700d3d986ea1a19</td>\n",
       "      <td>9d6f15f95d01e79bd1349cc208361f09</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>[mas, um, pouco, travando, pelo, valor, ta, boa]</td>\n",
       "      <td>[mas, um, pouco, travando, pelo, valor, ta, boa]</td>\n",
       "      <td>2018-02-16</td>\n",
       "      <td>2018-02-20 10:52:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d21bbc789670eab777d27372ab9094cc</td>\n",
       "      <td>4fc44d78867142c627497b60a7e0228a</td>\n",
       "      <td>5</td>\n",
       "      <td>[otimo]</td>\n",
       "      <td>[loja, nota]</td>\n",
       "      <td>[otimo, loja, nota]</td>\n",
       "      <td>2018-07-10</td>\n",
       "      <td>2018-07-11 14:10:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e0190b9db53b689b285d3f3916f8441</td>\n",
       "      <td>79832b7cb59ac6f887088ffd686e1d5e</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>[obrigado, pela, atencao, amim, dispensada]</td>\n",
       "      <td>[obrigado, pela, atencao, amim, dispensada]</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2017-12-09 22:58:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "1  8670d52e15e00043ae7de4c01cc2fe06  b9bf720beb4ab3728760088589c62129   \n",
       "2  4b49719c8a200003f700d3d986ea1a19  9d6f15f95d01e79bd1349cc208361f09   \n",
       "3  d21bbc789670eab777d27372ab9094cc  4fc44d78867142c627497b60a7e0228a   \n",
       "4  0e0190b9db53b689b285d3f3916f8441  79832b7cb59ac6f887088ffd686e1d5e   \n",
       "\n",
       "   review_score review_comment_title  \\\n",
       "0             5                   []   \n",
       "1             4          [recomendo]   \n",
       "2             4                   []   \n",
       "3             5              [otimo]   \n",
       "4             5                   []   \n",
       "\n",
       "                              review_comment_message  \\\n",
       "0  [parabens, lojas, lannister, adorei, comprar, ...   \n",
       "1  [aparelho, eficiente, no, site, marca, do, apa...   \n",
       "2   [mas, um, pouco, travando, pelo, valor, ta, boa]   \n",
       "3                                       [loja, nota]   \n",
       "4        [obrigado, pela, atencao, amim, dispensada]   \n",
       "\n",
       "                    review_comment_title_and_message review_creation_date  \\\n",
       "0  [parabens, lojas, lannister, adorei, comprar, ...           2018-03-01   \n",
       "1  [recomendo, aparelho, eficiente, no, site, mar...           2018-05-22   \n",
       "2   [mas, um, pouco, travando, pelo, valor, ta, boa]           2018-02-16   \n",
       "3                                [otimo, loja, nota]           2018-07-10   \n",
       "4        [obrigado, pela, atencao, amim, dispensada]           2017-12-01   \n",
       "\n",
       "  review_answer_timestamp  \n",
       "0     2018-03-02 10:26:53  \n",
       "1     2018-05-23 16:45:47  \n",
       "2     2018-02-20 10:52:22  \n",
       "3     2018-07-11 14:10:25  \n",
       "4     2017-12-09 22:58:58  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frzd_olist_order_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividindo entre conjunto de treino e teste de forma stratificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = frzd_olist_order_reviews.loc[:, ['review_comment_title_and_message', 'review_score']]\n",
    "y = frzd_olist_order_reviews.loc[:, ['review_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=51\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando tag para armazenar marcar cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = []\n",
    "\n",
    "for index, data in X_train.iterrows():\n",
    "    train_documents.append(TaggedDocument(data['review_comment_title_and_message'].tolist(), [str(data['review_score']) + '_star_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = []\n",
    "\n",
    "for index, data in X_test.iterrows():\n",
    "    test_documents.append(TaggedDocument(data['review_comment_title_and_message'].tolist(), [str(data['review_score']) + '_star_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo modelo Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 20:44:40,599 : INFO : using concatenative 6000-dimensional layer1\n",
      "2024-09-21 20:44:40,601 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/c,d400,n5,w7,mc3,s1e-05,t8>', 'datetime': '2024-09-21T20:44:40.601207', 'gensim': '4.3.3', 'python': '3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(\n",
    "    vector_size=400, # tamanho do vetor de embedding\n",
    "    min_count=3, # quantidade mínima de repetições palavras para entrarem no treinamento \n",
    "    sample=10**-5, # frequência teórica para ponderar palavras muito frequêntes\n",
    "    window=7,\n",
    "    shrink_windows=True,\n",
    "    hs=0, # utilizando amostragem negativa para acelarar o treinamento\n",
    "    negative=5, # determina que 20 palavras aleatórias serão utilizadas para treinar a palavra predita\n",
    "    dm=1, # usando modelo PV-DM\n",
    "    dm_concat=1, # concatenando vetor de parágrafo com de palavras\n",
    "    dbow_words=1, # treinando também vetor de palavras word2vec\n",
    "    workers=8, # numero sde cores utilizados no paralelismo\n",
    "    seed=51, # fixando a aleatoriada para deixar o modelo reprodutível\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 20:44:42,257 : INFO : collecting all words and their counts\n",
      "2024-09-21 20:44:42,261 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2024-09-21 20:44:42,348 : INFO : PROGRESS: at example #10000, processed 83266 words (1014113 words/s), 6819 word types, 5 tags\n",
      "2024-09-21 20:44:42,383 : INFO : collected 9668 word types and 5 unique tags from a corpus of 18721 examples and 157562 words\n",
      "2024-09-21 20:44:42,383 : INFO : Creating a fresh vocabulary\n",
      "2024-09-21 20:44:42,404 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 3297 unique words (34.10% of original 9668, drops 6371)', 'datetime': '2024-09-21T20:44:42.404599', 'gensim': '4.3.3', 'python': '3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-09-21 20:44:42,406 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 149855 word corpus (95.11% of original 157562, drops 7707)', 'datetime': '2024-09-21T20:44:42.406604', 'gensim': '4.3.3', 'python': '3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-09-21 20:44:42,448 : INFO : deleting the raw counts dictionary of 9668 items\n",
      "2024-09-21 20:44:42,449 : INFO : sample=1e-05 downsamples 2657 most-common words\n",
      "2024-09-21 20:44:42,449 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 21935.949367374047 word corpus (14.6%% of prior 149855)', 'datetime': '2024-09-21T20:44:42.449884', 'gensim': '4.3.3', 'python': '3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-09-21 20:44:42,490 : INFO : estimated required memory for 3297 words and 400 dimensions: 86060700 bytes\n",
      "2024-09-21 20:44:42,491 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 20:44:42,786 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 8 workers on 3298 vocabulary and 6000 features, using sg=0 hs=0 sample=1e-05 negative=5 window=7 shrink_windows=True', 'datetime': '2024-09-21T20:44:42.786958', 'gensim': '4.3.3', 'python': '3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-09-21 20:44:43,747 : INFO : EPOCH 0: training on 157562 raw words (40637 effective words) took 0.9s, 44565 effective words/s\n",
      "2024-09-21 20:44:44,625 : INFO : EPOCH 1: training on 157562 raw words (40673 effective words) took 0.8s, 48665 effective words/s\n",
      "2024-09-21 20:44:45,530 : INFO : EPOCH 2: training on 157562 raw words (40794 effective words) took 0.9s, 46281 effective words/s\n",
      "2024-09-21 20:44:46,349 : INFO : EPOCH 3: training on 157562 raw words (40620 effective words) took 0.8s, 51195 effective words/s\n",
      "2024-09-21 20:44:47,135 : INFO : EPOCH 4: training on 157562 raw words (40645 effective words) took 0.8s, 53787 effective words/s\n",
      "2024-09-21 20:44:47,909 : INFO : EPOCH 5: training on 157562 raw words (40457 effective words) took 0.7s, 54345 effective words/s\n",
      "2024-09-21 20:44:48,690 : INFO : EPOCH 6: training on 157562 raw words (40534 effective words) took 0.8s, 53242 effective words/s\n",
      "2024-09-21 20:44:49,442 : INFO : EPOCH 7: training on 157562 raw words (40585 effective words) took 0.7s, 54848 effective words/s\n",
      "2024-09-21 20:44:50,252 : INFO : EPOCH 8: training on 157562 raw words (40807 effective words) took 0.8s, 51892 effective words/s\n",
      "2024-09-21 20:44:51,035 : INFO : EPOCH 9: training on 157562 raw words (40676 effective words) took 0.8s, 54186 effective words/s\n",
      "2024-09-21 20:44:51,795 : INFO : EPOCH 10: training on 157562 raw words (40705 effective words) took 0.7s, 54633 effective words/s\n",
      "2024-09-21 20:44:52,558 : INFO : EPOCH 11: training on 157562 raw words (40489 effective words) took 0.7s, 54959 effective words/s\n",
      "2024-09-21 20:44:53,320 : INFO : EPOCH 12: training on 157562 raw words (40797 effective words) took 0.7s, 55028 effective words/s\n",
      "2024-09-21 20:44:54,077 : INFO : EPOCH 13: training on 157562 raw words (40650 effective words) took 0.7s, 55251 effective words/s\n",
      "2024-09-21 20:44:54,845 : INFO : EPOCH 14: training on 157562 raw words (40707 effective words) took 0.7s, 54934 effective words/s\n",
      "2024-09-21 20:44:55,615 : INFO : EPOCH 15: training on 157562 raw words (40651 effective words) took 0.8s, 54152 effective words/s\n",
      "2024-09-21 20:44:56,372 : INFO : EPOCH 16: training on 157562 raw words (40519 effective words) took 0.7s, 55586 effective words/s\n",
      "2024-09-21 20:44:57,117 : INFO : EPOCH 17: training on 157562 raw words (40639 effective words) took 0.7s, 56182 effective words/s\n",
      "2024-09-21 20:44:57,854 : INFO : EPOCH 18: training on 157562 raw words (40384 effective words) took 0.7s, 56511 effective words/s\n",
      "2024-09-21 20:44:58,600 : INFO : EPOCH 19: training on 157562 raw words (40584 effective words) took 0.7s, 55694 effective words/s\n",
      "2024-09-21 20:44:59,356 : INFO : EPOCH 20: training on 157562 raw words (40618 effective words) took 0.7s, 54724 effective words/s\n",
      "2024-09-21 20:45:00,113 : INFO : EPOCH 21: training on 157562 raw words (40510 effective words) took 0.7s, 54651 effective words/s\n",
      "2024-09-21 20:45:00,854 : INFO : EPOCH 22: training on 157562 raw words (40720 effective words) took 0.7s, 56935 effective words/s\n",
      "2024-09-21 20:45:01,592 : INFO : EPOCH 23: training on 157562 raw words (40777 effective words) took 0.7s, 56815 effective words/s\n",
      "2024-09-21 20:45:02,344 : INFO : EPOCH 24: training on 157562 raw words (40712 effective words) took 0.7s, 56049 effective words/s\n",
      "2024-09-21 20:45:03,109 : INFO : EPOCH 25: training on 157562 raw words (40768 effective words) took 0.7s, 55422 effective words/s\n",
      "2024-09-21 20:45:03,896 : INFO : EPOCH 26: training on 157562 raw words (40575 effective words) took 0.8s, 53043 effective words/s\n",
      "2024-09-21 20:45:04,652 : INFO : EPOCH 27: training on 157562 raw words (40808 effective words) took 0.7s, 56183 effective words/s\n",
      "2024-09-21 20:45:05,383 : INFO : EPOCH 28: training on 157562 raw words (40685 effective words) took 0.7s, 57461 effective words/s\n",
      "2024-09-21 20:45:06,123 : INFO : EPOCH 29: training on 157562 raw words (40444 effective words) took 0.7s, 56641 effective words/s\n",
      "2024-09-21 20:45:06,893 : INFO : EPOCH 30: training on 157562 raw words (40607 effective words) took 0.8s, 53868 effective words/s\n",
      "2024-09-21 20:45:07,641 : INFO : EPOCH 31: training on 157562 raw words (40494 effective words) took 0.7s, 56337 effective words/s\n",
      "2024-09-21 20:45:08,388 : INFO : EPOCH 32: training on 157562 raw words (40663 effective words) took 0.7s, 56116 effective words/s\n",
      "2024-09-21 20:45:09,131 : INFO : EPOCH 33: training on 157562 raw words (40783 effective words) took 0.7s, 56866 effective words/s\n",
      "2024-09-21 20:45:09,856 : INFO : EPOCH 34: training on 157562 raw words (40586 effective words) took 0.7s, 57792 effective words/s\n",
      "2024-09-21 20:45:10,599 : INFO : EPOCH 35: training on 157562 raw words (40827 effective words) took 0.7s, 57027 effective words/s\n",
      "2024-09-21 20:45:11,347 : INFO : EPOCH 36: training on 157562 raw words (40713 effective words) took 0.7s, 56025 effective words/s\n",
      "2024-09-21 20:45:12,103 : INFO : EPOCH 37: training on 157562 raw words (40501 effective words) took 0.7s, 55171 effective words/s\n",
      "2024-09-21 20:45:12,844 : INFO : EPOCH 38: training on 157562 raw words (40757 effective words) took 0.7s, 55977 effective words/s\n",
      "2024-09-21 20:45:13,588 : INFO : EPOCH 39: training on 157562 raw words (40705 effective words) took 0.7s, 56140 effective words/s\n",
      "2024-09-21 20:45:14,332 : INFO : EPOCH 40: training on 157562 raw words (40313 effective words) took 0.7s, 55312 effective words/s\n",
      "2024-09-21 20:45:15,068 : INFO : EPOCH 41: training on 157562 raw words (40545 effective words) took 0.7s, 56148 effective words/s\n",
      "2024-09-21 20:45:15,806 : INFO : EPOCH 42: training on 157562 raw words (40601 effective words) took 0.7s, 55896 effective words/s\n",
      "2024-09-21 20:45:16,538 : INFO : EPOCH 43: training on 157562 raw words (40569 effective words) took 0.7s, 57120 effective words/s\n",
      "2024-09-21 20:45:17,403 : INFO : EPOCH 44: training on 157562 raw words (40697 effective words) took 0.8s, 48332 effective words/s\n",
      "2024-09-21 20:45:18,153 : INFO : EPOCH 45: training on 157562 raw words (40770 effective words) took 0.7s, 56894 effective words/s\n",
      "2024-09-21 20:45:18,887 : INFO : EPOCH 46: training on 157562 raw words (40572 effective words) took 0.7s, 56344 effective words/s\n",
      "2024-09-21 20:45:19,631 : INFO : EPOCH 47: training on 157562 raw words (40555 effective words) took 0.7s, 56474 effective words/s\n",
      "2024-09-21 20:45:20,366 : INFO : EPOCH 48: training on 157562 raw words (40584 effective words) took 0.7s, 56409 effective words/s\n",
      "2024-09-21 20:45:21,102 : INFO : EPOCH 49: training on 157562 raw words (40502 effective words) took 0.7s, 55911 effective words/s\n",
      "2024-09-21 20:45:21,864 : INFO : EPOCH 50: training on 157562 raw words (40733 effective words) took 0.7s, 55114 effective words/s\n",
      "2024-09-21 20:45:22,615 : INFO : EPOCH 51: training on 157562 raw words (40691 effective words) took 0.7s, 55086 effective words/s\n",
      "2024-09-21 20:45:23,367 : INFO : EPOCH 52: training on 157562 raw words (40689 effective words) took 0.7s, 55840 effective words/s\n",
      "2024-09-21 20:45:24,111 : INFO : EPOCH 53: training on 157562 raw words (40664 effective words) took 0.7s, 57074 effective words/s\n",
      "2024-09-21 20:45:24,840 : INFO : EPOCH 54: training on 157562 raw words (40639 effective words) took 0.7s, 57982 effective words/s\n",
      "2024-09-21 20:45:25,584 : INFO : EPOCH 55: training on 157562 raw words (40604 effective words) took 0.7s, 56681 effective words/s\n",
      "2024-09-21 20:45:26,323 : INFO : EPOCH 56: training on 157562 raw words (40766 effective words) took 0.7s, 57281 effective words/s\n",
      "2024-09-21 20:45:27,077 : INFO : EPOCH 57: training on 157562 raw words (40514 effective words) took 0.7s, 55293 effective words/s\n",
      "2024-09-21 20:45:27,816 : INFO : EPOCH 58: training on 157562 raw words (40674 effective words) took 0.7s, 57066 effective words/s\n",
      "2024-09-21 20:45:28,560 : INFO : EPOCH 59: training on 157562 raw words (40839 effective words) took 0.7s, 55969 effective words/s\n",
      "2024-09-21 20:45:29,288 : INFO : EPOCH 60: training on 157562 raw words (40602 effective words) took 0.7s, 57672 effective words/s\n",
      "2024-09-21 20:45:30,025 : INFO : EPOCH 61: training on 157562 raw words (40628 effective words) took 0.7s, 56670 effective words/s\n",
      "2024-09-21 20:45:30,790 : INFO : EPOCH 62: training on 157562 raw words (40603 effective words) took 0.7s, 54178 effective words/s\n",
      "2024-09-21 20:45:31,527 : INFO : EPOCH 63: training on 157562 raw words (40583 effective words) took 0.7s, 56795 effective words/s\n",
      "2024-09-21 20:45:32,282 : INFO : EPOCH 64: training on 157562 raw words (40665 effective words) took 0.7s, 54805 effective words/s\n",
      "2024-09-21 20:45:33,026 : INFO : EPOCH 65: training on 157562 raw words (40683 effective words) took 0.7s, 56224 effective words/s\n",
      "2024-09-21 20:45:33,753 : INFO : EPOCH 66: training on 157562 raw words (40673 effective words) took 0.7s, 57650 effective words/s\n",
      "2024-09-21 20:45:34,499 : INFO : EPOCH 67: training on 157562 raw words (40603 effective words) took 0.7s, 56239 effective words/s\n",
      "2024-09-21 20:45:35,307 : INFO : EPOCH 68: training on 157562 raw words (40738 effective words) took 0.8s, 52115 effective words/s\n",
      "2024-09-21 20:45:36,144 : INFO : EPOCH 69: training on 157562 raw words (40627 effective words) took 0.8s, 50403 effective words/s\n",
      "2024-09-21 20:45:36,146 : INFO : Doc2Vec lifecycle event {'msg': 'training on 11029340 raw words (2844732 effective words) took 53.4s, 53315 effective words/s', 'datetime': '2024-09-21T20:45:36.146439', 'gensim': '4.3.3', 'python': '3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_documents,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=70\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste de 'sanidade', calculando similaridade de documentos em relação ao conjunto todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ranks = []\n",
    "\n",
    "for document in train_documents:\n",
    "    inferred_vector = model.infer_vector(document.words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(document.tags[0])\n",
    "    train_ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 12932, 1: 3471, 2: 1259, 3: 653, 4: 406})"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(train_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x de 18721 avaliações (x %) são similares a elas mesmas, o que é um bom sinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ranks = []\n",
    "\n",
    "for document in test_documents:\n",
    "    inferred_vector = model.infer_vector(document.words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(document.tags[0])\n",
    "    test_ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2736, 1: 812, 2: 470, 3: 365, 4: 298})"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(test_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x de 4681 avaliações (x %) são similares a elas mesmas, o que é um bom sinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (55) | tag (4_star_score): «muito boa»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/c,d400,n5,hs,w7,mc3,s1e-05,t8>:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('4_star_score', 0.6903237104415894),\n",
       " ('5_star_score', 0.5428512096405029),\n",
       " ('3_star_score', 0.28243574500083923),\n",
       " ('2_star_score', -0.06443776190280914),\n",
       " ('1_star_score', -0.2582360506057739)]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = 55\n",
    "\n",
    "inferred_vector = model.infer_vector(train_documents[doc_id].words)\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "print('Document ({}) | tag ({}): «{}»\\n'.format(doc_id, train_documents[doc_id].tags[0], ' '.join(train_documents[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "# for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "#     # print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_documents[sims[index][0]].words)))\n",
    "#     print()\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST (5807, -0.22642099857330322): «suporta ate»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "semantic_query = 'estou com problemas com meu numero da claro'\n",
    "\n",
    "inferred_vector = model.infer_vector(semantic_query.split(' '))\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "print(u'%s %s: «%s»\\n' % ('MOST', sims[index], ' '.join(train_documents[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando o modelo com o conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (3221): «otimo»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/c,d300,n20,w5,mc5,s1e-05,t8>:\n",
      "\n",
      "MOST (8283, 0.22291600704193115): «muito rui eles nao ciporta com criente»\n",
      "\n",
      "MEDIAN (12816, 0.0006949880626052618): «nota»\n",
      "\n",
      "LEAST (13287, -0.24041034281253815): «toner ja esta instalado funcionando perfeitamente»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escolhendo aleatoriamente um documento do conjunto de teste\n",
    "doc_id = random.randint(0, len(test_documents) - 1)\n",
    "inferred_vector = model.infer_vector(test_documents[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Comparando este documento com o conjunto de treinamento\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_documents[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_documents[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando modelo Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 19:02:23,668 : INFO : Doc2Vec lifecycle event {'fname_or_handle': '../model/doc2vec_telecom_pandemic_claims', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-09-19T19:02:23.667075', 'gensim': '4.3.3', 'python': '3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'saving'}\n",
      "2024-09-19 19:02:23,672 : INFO : storing np array 'syn1neg' to ../model/doc2vec_telecom_pandemic_claims.syn1neg.npy\n",
      "2024-09-19 19:02:23,999 : INFO : not storing attribute cum_table\n",
      "2024-09-19 19:02:24,126 : INFO : saved ../model/doc2vec_telecom_pandemic_claims\n"
     ]
    }
   ],
   "source": [
    "model.save('../model/doc2vec_telecom_pandemic_claims')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eEDB-002-2024-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
